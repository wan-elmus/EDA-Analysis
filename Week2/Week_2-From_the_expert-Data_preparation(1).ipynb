{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "phantom-danish",
   "metadata": {},
   "source": [
    "# From the expert: data cleaning and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-scratch",
   "metadata": {},
   "source": [
    "In the [CRISP-DM](https://www.datascience-pm.com/crisp-dm-2/) framework, we are in step 3:\n",
    "\n",
    "[](attachment:image.png)\n",
    "<img src='attachment:image.png' width=400>\n",
    "\n",
    "1. **Business understanding (determining our goals and purpose of the project)**\n",
    "\n",
    "Can we predict the occurance of diabetes based on health and biographic data?\n",
    "\n",
    "2. **Data Understanding**\n",
    "\n",
    "Week 1 - EDA and visualization\n",
    "\n",
    "3. **Data Preparation**\n",
    "\n",
    "This is where we are in week 2. Once we have an understanding of the data, we can clean and prepare it for statistical or mathematical analysis. This includes things like sampling the data, cleaning missing values and outliers, and feature engineering. Steps 2 and 3 can be intermixed, because we often want to convert as much data to numeric datatypes as possible before doing EDA.\n",
    "\n",
    "4. **Modeling**\n",
    "\n",
    "Next week.\n",
    "\n",
    "5. **Evaluation**\n",
    "\n",
    "Next week.\n",
    "\n",
    "6. **Deployment**\n",
    "\n",
    "Next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-eclipse",
   "metadata": {},
   "source": [
    "## What we'll cover here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatty-islam",
   "metadata": {},
   "source": [
    "**1. Filtering data with pandas**\n",
    "\n",
    "\n",
    "**2. Checking for outliers**\n",
    "- deal with them: drop, clip, or treat as missing\n",
    "\n",
    "**2. Missing values**\n",
    "- check for them, deal with them\n",
    "- drop rows with missing values\n",
    "- imputation - fill with median, mean, mode, or ML\n",
    "\n",
    "**3. Convert categorical to numeric**\n",
    "- binary encoding\n",
    "- label encoding\n",
    "- one-hot encoding\n",
    "\n",
    "**4. Feature engineering**\n",
    "- mathematical transforms (standardize, Yeo-Johnson, log transform, etc)\n",
    "- combining features\n",
    "- extracting features from datetimes\n",
    "\n",
    "Some of the above topics will be covered in the optional advanced section at the bottom of the notebook.\n",
    "\n",
    "### Other topics we won't cover here, but are part of data cleaning\n",
    "\n",
    "- looking for odd values that signify or should be missing values\n",
    "    - e.g. -999, -1, and sometimes 0s are actually missing values\n",
    "- cleaning inconsistencies in categorical columns\n",
    "    - e.g. Male, male, and MALE could all be in a gender column but should all be mapped to a consistent value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-lincoln",
   "metadata": {},
   "source": [
    "# 3. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "based-equality",
   "metadata": {},
   "source": [
    "First we load the data as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-barrel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-university",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can give an index number or name for our index column, or leave it blank\n",
    "df = pd.read_excel('diabetes_data.xlsx', index_col='Patient number')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-immunology",
   "metadata": {},
   "source": [
    "## Filtering data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-source",
   "metadata": {},
   "source": [
    "You may have already gone through this last week, in which case, you can skip this section.\n",
    "\n",
    "We can create a boolean array with a comparison operator, then filter our dataframe like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "architectural-rapid",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes_df = df[df['Diabetes'] == 'Diabetes']\n",
    "diabetes_df['Diabetes'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-airport",
   "metadata": {},
   "source": [
    "This allows us to plot subsets of the data or to change certain parts of the data. We can use all the other boolean comparison operators, like >, <, >=, !=, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "systematic-supplement",
   "metadata": {},
   "outputs": [],
   "source": [
    "older_df = df[df['Age'] == 30]\n",
    "older_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409820b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "False == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earlier-apple",
   "metadata": {},
   "source": [
    "## Checking for outliers and dealing with them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-diagram",
   "metadata": {},
   "source": [
    "Outliers are datapoints that are atypical and far outside of the region of normal data, and can affect machine learning models and statistical analyses in negative ways. Sometimes the outliers are mistakes in the data and should be cleaned, other times they are abnormal or atypical datapoints. Depending on what we think is the case, we can throw out datapoints with outliers, treat them as missing values, or clip them to max and min values.\n",
    "\n",
    "Classic ways of detecting outliers are the IQR (inter-quartile range) method, z-score (percentiles), or using standard deviations. These rely on the assumption the data is normally distributed (e.g. in a bell curve shape on the histogram). Boxplots use IQR to draw the whiskers, and we can use those to first see if it looks like there are outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "little-minister",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.boxplot(data=df, orient='h')\n",
    "#ax.set_xscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-treasure",
   "metadata": {},
   "source": [
    "Yikes, it looks like a lot of outliers for almost all the numeric columns! Using a standard deviation or IQR outlier detection like this relies on our data being near a normal distribution, or bell-shaped. Since much of it is not, and it looks like we have a lot of outliers in all columns, we are not going to take action with these outliers. But if we wanted to take action with the outliers, we could do something like the following: loop through each column, calculate the IQR boundaries, and then do something with these outliers. Here is an example of examining some of the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improved-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = 'Cholesterol'\n",
    "q1 = df[column].quantile(0.25)\n",
    "q3 = df[column].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "upper_boundary = q3 + 1.5 * iqr\n",
    "lower_boundary = q1 - 1.5 * iqr\n",
    "df[(df[column] < lower_boundary) | (df[column] > upper_boundary)][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4741120",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['Cholesterol'].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-caribbean",
   "metadata": {},
   "source": [
    "One option: set values as missing. Then we can fill them or drop the values as needed in the next section.\n",
    "\n",
    "The `.at` indexer for pandas allows us to set values by providing row selections and a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# make a copy so as to to alter the original data\n",
    "df_copy = df.copy().reset_index()\n",
    "df_copy.loc[df[column] < lower_boundary, column] = np.nan\n",
    "df_copy.loc[df[column] > upper_boundary, column] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-filter",
   "metadata": {},
   "source": [
    "Another option: clip values to outlier boundaries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-hepatitis",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy[column].clip(lower=lower_boundary, upper=upper_boundary, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excellent-boost",
   "metadata": {},
   "source": [
    "We can see this clipping removed outliers from our boxplot, but of course we altered the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=df_copy, orient='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extra-upset",
   "metadata": {},
   "source": [
    "In our case, we will assume the data is OK and the outliers are simply a function of the small amount of data and the erratic nature of biological measurements, especially with people who have diabetes. For example, the glucose measurements for diabetics are all over the map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-consolidation",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-administration",
   "metadata": {},
   "source": [
    "Similar to outliers, we can deal with missing values in a few ways: drop the data, or fill it. We can fill the data (impute it) with a few methods:\n",
    "- mean: good when distrubtion is near normal (like the height of people in a city)\n",
    "- median: works well when distribution is skewed (like housing prices)\n",
    "- mode: good for categorical data\n",
    "- machine learning: good for complex situations or to maximize the effect of your data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = df.copy()\n",
    "missing.loc[df['Age'] == 30, 'Age'] = np.nan\n",
    "missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forty-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this would drop any rows with at least 1 missing value\n",
    "missing.dropna(inplace = True)\n",
    "missing.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "severe-backup",
   "metadata": {},
   "source": [
    "If we had some missing Glucose values, we might fill those with the median:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-actor",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Glucose'].fillna(df['Glucose'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-wyoming",
   "metadata": {},
   "source": [
    "However, remember the distributions were very different between the diabetic and non-diabetic populations. So, ideally, we would fill the missing values for the diabetics and non-diabetics separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-ivory",
   "metadata": {},
   "source": [
    "## Converting categorical variables to numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "generous-scenario",
   "metadata": {},
   "source": [
    "For using the `sklearn` machine learning library, we need all data as numeric types, but we have two `object` datatypes which are strings. There are many ways to convert a string column to a numeric column. If the values are `'True'`/`'False'`, we can use `.astype('int')`. Otherwise, a few ways are to use pandas functions like `map`, `replace`, and `apply`. `map` is the most computationally efficient usually, but replace is a little more forgiving and flexible. `map` will change values to NaN if they don't match any keys in the dictionary we provide, whereas replace can replace part or all of the data. However, `map` is computationally faster (it runs faster) so would work better for bigger data or in a production setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-technique",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diabetes'] = df['Diabetes'].replace({'No diabetes': 0, 'Diabetes': 1})\n",
    "df['Diabetes'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spanish-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].replace({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-universe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that all columns are numbers now\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civic-savannah",
   "metadata": {},
   "source": [
    "If we had more than 2 categories, we can simply add more entries to our dictionary (or use other methods in the advanced section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imported-cleaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Diabetes'] = df['Diabetes'].replace({'No diabetes': 0, 'Pre diabetes': 1, 'Diabetes': 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-israel",
   "metadata": {},
   "source": [
    "## Feature engineering - combining features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fallen-teach",
   "metadata": {},
   "source": [
    "Feature engineering can be an important part of data science when using machine learning. The features, or inputs, we provide to our ML algorithm will influence its performance. A few feature engineering techniques are:\n",
    "\n",
    "- mathematical transforms (log, Yeo-Johnson, etc)\n",
    "- combining columns\n",
    "- extracting features from datetimes\n",
    "\n",
    "We'll look at scaling data with a log transform and combining columns here.\n",
    "\n",
    "To scale data with a log transform, we can simply use numpy. This transform can be useful for highly skewed data, like our HDL cholesterol measurements. You can read more about the transform [here](https://onlinestatbook.com/2/transformations/log.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-protection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "df_copy = df.copy()\n",
    "df_copy['HDL Chol'] = np.log(df_copy['HDL Chol'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "british-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy['HDL Chol'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['HDL Chol'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-southwest",
   "metadata": {},
   "source": [
    "We can see how it makes the distribution look more like a normal distribution or bell-curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-pregnancy",
   "metadata": {},
   "source": [
    "To combine columns, we simply use normal math. For example, we can create a waist/hip ratio and HDL to total cholesterol ratio, which can be useful in diabetes studies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-speaker",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['waist_hip_ratio'] = df['waist'] / df['hip']\n",
    "df['hdl_chol_ratio'] = df['HDL Chol'] / df['Cholesterol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-instrument",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['waist_hip_ratio'].plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hdl_chol_ratio'].plot.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-canon",
   "metadata": {},
   "source": [
    "## Checking results and saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-coffee",
   "metadata": {},
   "source": [
    "Now that we have our data cleaned up and prepared, we can save it for future use. Let's give it one last check with `info`, then save it to a CSV. There are many other formats in which we can save the data, which you can see in the [documentation](https://pandas.pydata.org/docs/reference/io.html) for pandas. Almost any filetype with a `read_x` function has a `to_x` method for dataframes as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-patent",
   "metadata": {},
   "source": [
    "The data looks good - we have all columns as numeric datatypes and no missing values. We will now save it to a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minor-china",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('prepped_diabetes_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distant-creature",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "running-bruce",
   "metadata": {},
   "source": [
    "Here, we loaded, cleaned, and feature engineered the diabetes dataset. We first inspected the dataset for outliers using the IQR method, and found several outliers. This is likely due to the small amount of data. Because few of the outliers were very isolated from other datapoints and because there were so many outliers, we elected to leave them as-is. We did not find any missing values in the data. We converted the categorical columns to numeric columns with binary label encoding. We created two new features, the waist/hip ratio and HDL/cholesterol ratio. Finally, we saved our data as a CSV and it is ready for the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-google",
   "metadata": {},
   "source": [
    "# (optional) Advanced section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "magnetic-practice",
   "metadata": {},
   "source": [
    "We will cover:\n",
    "- Advanced outlier detection\n",
    "- filling missing values with ML\n",
    "- Converting categorical to numeric\n",
    "    - sklearn labelencoder\n",
    "    - one-hot encoding\n",
    "- Yeo-Johnson transform\n",
    "\n",
    "Although we won't cover it, the `missingno` package is a nice one for visualizing missing values in a dataframe. It's also how pandas-profiling draws some of the missing value plots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-native",
   "metadata": {},
   "source": [
    "### Advanced outlier detection and feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-variance",
   "metadata": {},
   "source": [
    "Outlier detection is more involved than you might think. There are new outlier algorithms constantly being developed, and it's an active area of research. The pyod package in Python has many of the cutting-edge algorithms. One problem with many of the cutting-edge algorithms is we need to specify a 'contamination' proportion, which specifies the proportion of outliers. This is usually set by an expectation for the number of outliers that we may know or estimate. The KNN method, which uses distances between points to predict outliers, runs fast and we will use it here. Since it's a distance-based algorithm, it's best to scale our data first before trying to detect outliers with it. However, the scaling doesn't seem to make a big difference here. We can also only use numeric data, so we select those columns first. Let's re-load the data so we're starting from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-massachusetts",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('diabetes_data.xlsx', index_col='Patient number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-polls",
   "metadata": {},
   "source": [
    "Then we scale it so the standard deviation of all columns are 1 and the means of all columns are 0, called standardization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-album",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_numeric = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e51ab20",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = scaled_numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-blame",
   "metadata": {},
   "source": [
    "We set contamination very low so we only get the most extreme values. We do need to install the pyod package before running this next code section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pyod -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-cheese",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "od = KNN(contamination=0.01)\n",
    "od.fit(scaled_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-adjustment",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = od.predict(scaled_numeric)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-ukraine",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-object",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[outliers.astype('bool')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unknown-width",
   "metadata": {},
   "source": [
    "Now this gives us something to think about. These outliers tend to have one or more measurements on the extreme. For example, the last person seems to be an older woman with a low BMI and a high systolic blood pressure. However, this is again medical data, and the reason we probably have some outliers here is the small amount of data collected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-realtor",
   "metadata": {},
   "source": [
    "### Filling missing values with ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-balance",
   "metadata": {},
   "source": [
    "The [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) from sklearn is fairly easy to use. We simply create the imputer and use the fit_transform method. There are some parameters for the method we could tune as well to try and improve performance. All values going in need to be numeric for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf132a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing = numeric_df.copy()\n",
    "df_missing.loc[df['hip'] == 39, 'hip'] = np.nan\n",
    "#df_missing.head()\n",
    "#df_missing.info()\n",
    "df_missing.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-reserve",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer()\n",
    "filled_values = imputer.fit_transform(df_missing)\n",
    "filled_df = pd.DataFrame(data=filled_values, columns=numeric_df.columns, index=numeric_df.index)\n",
    "\n",
    "obj_df = df.select_dtypes(include=['object'])\n",
    "\n",
    "# merge the two dfs back into one\n",
    "full_df = pd.concat([filled_df, obj_df], axis=1)\n",
    "full_df.head()\n",
    "full_df.info()\n",
    "full_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42749e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hip'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb327001",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df['hip'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-bearing",
   "metadata": {},
   "source": [
    "### Other methods to convert categorical to numeric data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concrete-hollywood",
   "metadata": {},
   "source": [
    "If we have many categorical values, we can use sklearn's label encoder to preprocess them. The examples in the documentation show how to do this: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "diabetes_le = le.fit_transform(df['Diabetes'])\n",
    "\n",
    "print(diabetes_le)\n",
    "print(le.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-steam",
   "metadata": {},
   "source": [
    "We can also use one-hot encoding for multi-category variables. This is fairly easy with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['Gender'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-resource",
   "metadata": {},
   "source": [
    "With more than 2 values, it will create more columns. Since we can always infer one of the columns from all others (e.g. if all others are 0, we know the last column should be a 1), we can drop one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['Gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-airline",
   "metadata": {},
   "source": [
    "We can then combine this with the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-livestock",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_df = pd.concat([df.drop('Gender', axis=1), pd.get_dummies(df['Gender'], drop_first=True)],axis=1)\n",
    "one_hot_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focused-democrat",
   "metadata": {},
   "source": [
    "### The Yeo-Johnson transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-spirituality",
   "metadata": {},
   "source": [
    "Much like the log transform can convert our data into a normal-looking (Gaussian or bell-curve) distribution, the Yeo-Johnson can do the same. However, the YJ method is a bit more optimized and advanced. The [Wikipedia page](https://en.wikipedia.org/wiki/Power_transform#Yeo%E2%80%93Johnson_transformation) shows the math behind it. We can use it in Python like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-information",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer()\n",
    "df['yj_HDL'] = pt.fit_transform(df[['HDL Chol']])\n",
    "df[['yj_HDL', 'HDL Chol']].plot.density(subplots=True, sharex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-supply",
   "metadata": {},
   "source": [
    "We can see the YJ-transformed data centers around 0 (it has also been standardized with `StandardScaler`) and looks much more symmetric and normal than the unmodified data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8484825b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
